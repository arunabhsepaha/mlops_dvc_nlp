NLP_USE_CASE.
=================

rest of the files got from zip of below link :- 
https://github.com/arpanghosh130883/dvc-project-template

git repo need to be created in mine or forked above one and then do git init in my local
 -- currently not done. 

dvc_nlp
requirements.txt
setup.py
data
data/data/xml # from sharepoint of visualpath under DVC_NLP
config/config.yaml
src/stage_01_prepare.py # rename stage_00_template.py
src/utils/common.py
src/utils/data_mgmt.py
src/utils/featurize.py

----------------------------

setup.py

replace author, email, git url :- 


from setuptools import setup
 
with open("README.md", "r", encoding="utf-8") as f:
    long_description = f.read()
 
## edit below variables as per your requirements -
REPO_NAME = "DVC-NLP-Simple-usecase"
AUTHOR_USER_NAME = "XXX"
SRC_REPO = "src"
LIST_OF_REQUIREMENTS = []
 
 
setup(
    name=SRC_REPO,
    version="0.0.1",
    author=AUTHOR_USER_NAME,
    description="A small package for DVC-NLP",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url=f"https://github.com/{AUTHOR_USER_NAME}/{REPO_NAME}",
    author_email="XXXX@gmail.com",
    packages=[SRC_REPO],
    license="MIT",
    python_requires=">=3.6",
    install_requires=LIST_OF_REQUIREMENTS
)


----------------------------

src/utils/featurize.py

import os
import logging
import pandas as pd
import joblib
import scipy.sparse as sparse
import numpy as np
 
def save_matrix(df, matrix, out_path):
    id_matrix = sparse.csr_matrix(df.id.astype(np.int64)).T
    label_matrix = sparse.csr_matrix(df.label.astype(np.int64)).T
 
    result = sparse.hstack([id_matrix, label_matrix, matrix], format="csr")
 
    msg = f"The ouput matrix {out_path} of size {result.shape} and data type: {result.dtype}"
    logging.info(msg)
    joblib.dump(result, out_path)

----------------------------
src\utils\data_mgmt.py

import logging
from tqdm import tqdm
import random
import xml.etree.ElementTree as ET
import re
 
def process_posts(fd_in, fd_out_train, fd_out_test, target_tag, split):
 
    line_num = 1
    for line in tqdm(fd_in):
        try:
            fd_out = fd_out_train if random.random() > split else fd_out_test
            attr = ET.fromstring(line).attrib
 
            pid = attr.get("Id", "")
            label = 1 if target_tag in attr.get("Tags", "") else 0
            title = re.sub(r"\s+", " ", attr.get("Title", "")).strip()
            body = re.sub(r"\s+", " ", attr.get("Body", "")).strip()
            text = title + " " + body
 
            fd_out.write(f"{pid}\t{label}\t{text}\n")
            line_num += 1
        except Exception as e:
            msg = f"Skipping the broken line {line_num}: {e}\n"
            logging.exception(msg)

----------------------------
src\utils\common.py

replace above file content with below.

import os
import yaml
import logging
import time
import pandas as pd
import json
 
def read_yaml(path_to_yaml: str) -> dict:
    with open(path_to_yaml) as yaml_file:
        content = yaml.safe_load(yaml_file)
    logging.info(f"yaml file: {path_to_yaml} loaded successfully")
    return content
 
def create_directories(path_to_directories: list) -> None:
    for path in path_to_directories:
        os.makedirs(path, exist_ok=True)
        logging.info(f"created directory at: {path}")
 
def get_df(path_to_data: str, sep: str="\t") -> pd.DataFrame:
    df = pd.read_csv(
        path_to_data,
        encoding="utf-8",
        header=None,
        delimiter=sep,
        names=["id", "label", "text"],
    )
    logging.info(f"The input data frame {path_to_data} size is {df.shape}\n")
    return df
 
def save_json(path, data):
    with open(path, "w") as f:
        json.dump(data, f, indent=4)
 
    logging.info(f"json file saved at: {path}")

----------------------------

src/stage_01_prepare.py

import argparse
import os
import shutil
from tqdm import tqdm
import logging
from src.utils.common import read_yaml, create_directories
from src.utils.data_mgmt import process_posts
import random


STAGE = "One"

logging.basicConfig(
    filename=os.path.join("logs", 'running_logs.log'), 
    level=logging.INFO, 
    format="[%(asctime)s: %(levelname)s: %(module)s]: %(message)s",
    filemode="a"
    )

def main(config_path, params_path):
    ## converting XML data tsv
    config = read_yaml(config_path)
    params = read_yaml(params_path)

    source_data = config["source_data"]
    input_data = os.path.join(source_data["data_dir"], source_data["data_file"])

    split = params["prepare"]["split"]
    seed = params["prepare"]["seed"]

    random.seed(seed)

    artifacts = config["artifacts"]
    prepared_data_dir_path = os.path.join(artifacts["ARTIFACTS_DIR"], artifacts["PREPARED_DATA"])
    create_directories([prepared_data_dir_path])

    train_data_path = os.path.join(prepared_data_dir_path, artifacts["TRAIN_DATA"])
    test_data_path = os.path.join(prepared_data_dir_path, artifacts["TEST_DATA"])
    
    encode = "utf8"
    with open(input_data, encoding=encode) as fd_in:
        with open(train_data_path, "w", encoding=encode) as fd_out_train:
            with open(test_data_path, "w", encoding=encode) as fd_out_test:
                process_posts(fd_in, fd_out_train, fd_out_test, "<python>", split)




if _name_ == '_main_':
    args = argparse.ArgumentParser()
    args.add_argument("--config", "-c", default="configs/config.yaml")
    args.add_argument("--params", "-p", default="params.yaml")
    parsed_args = args.parse_args()

    try:
        logging.info("\n********")
        logging.info(f">>>>> stage {STAGE} started <<<<<")
        main(config_path=parsed_args.config, params_path=parsed_args.params)
        logging.info(f">>>>> stage {STAGE} completed!<<<<<\n")
    except Exception as e:
        logging.exception(e)
        raise e

----------------------------
config.yaml

replace it with below content :- 

# It contains all the configs required in the project
 
artifacts:
  ARTIFACTS_DIR: artifacts
  PREPARED_DATA: prepared
  TRAIN_DATA: train.tsv
  TEST_DATA: test.tsv
  FEATURIZED_DATA: features
  FEATURIZED_OUT_TRAIN: train.pkl
  FEATURIZED_OUT_TEST: test.pkl
  MODEL_DIR: model
  MODEL_NAME: model.pkl
 
 
metrics:
  SCORES: scores.json
 
plots:
  PRC: prc.json
  ROC: roc.json
 
source_data:
  data_dir: data
  data_file: data.xml

----------------------------
requirements.txt

### dependency
tqdm
dvc
dvc[s3]
pandas
numpy
Scipy
pyyaml
scikit-learn
lxml
botocore
-e .
-----------------------------

Terminal :- 
=========

conda create --prefix ./env python==3.8 -y

conda activate /s/MLOPS/dvc_nlp/env/

pip install -r requirements.txt


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
19-JUNE_2025

dvc.yaml
params.yaml
src/stage_02_featurization.py
src/stage_03_train.py
src/stage_04_evaluate.py

-----------------------------
src/stage_04_evaluate.py

import argparse
import os
import shutil
from tqdm import tqdm
import logging
from src.utils.common import read_yaml, save_json
import joblib
import numpy as np
import sklearn.metrics as metrics
import math
 
 
STAGE = "Four"
 
logging.basicConfig(
    filename=os.path.join("logs", 'running_logs.log'), 
    level=logging.INFO, 
    format="[%(asctime)s: %(levelname)s: %(module)s]: %(message)s",
    filemode="a"
    )
 
 
def main(config_path):
    config = read_yaml(config_path)
 
    artifacts = config["artifacts"]
    featurized_data_dir_path = os.path.join(artifacts["ARTIFACTS_DIR"], artifacts["FEATURIZED_DATA"])
    featurized_test_data_path = os.path.join(featurized_data_dir_path, artifacts["FEATURIZED_OUT_TEST"])
 
    model_dir_path = os.path.join(artifacts["ARTIFACTS_DIR"], artifacts["MODEL_DIR"])
    model_path = os.path.join(model_dir_path, artifacts["MODEL_NAME"])
 
    model = joblib.load(model_path)
    matrix = joblib.load(featurized_test_data_path)
 
    labels = np.squeeze(matrix[:, 1].toarray())
    X = matrix[:,2:]
 
    predictions_by_class = model.predict_proba(X)
    predictions = predictions_by_class[:, 1]
 
    PRC_json_path = config["plots"]["PRC"]
    ROC_json_path = config["plots"]["ROC"]
    scores_json_path = config["metrics"]["SCORES"]
 
    avg_prec = metrics.average_precision_score(labels, predictions)
    roc_auc = metrics.roc_auc_score(labels, predictions)
 
    scores = {
        "avg_prec": avg_prec,
        "roc_auc": roc_auc
    }
    save_json(scores_json_path, scores)
 
    precision, recall, prc_threshold = metrics.precision_recall_curve(labels, predictions)
    nth_point = math.ceil(len(prc_threshold)/1000)
    prc_points = list(zip(precision, recall, prc_threshold))[::nth_point]
 
    prc_data = {
        "prc": [
            {"precision": p, "recall": r, "threshold": t}
            for p, r, t in prc_points
        ]
    }
    save_json(PRC_json_path, prc_data)
    fpr, tpr, roc_threshold = metrics.roc_curve(labels, predictions)
 
    roc_data = {
        "roc": [
            {"fpr": fp, "tpr": tp, "threshold": t}
            for fp, tp, t in zip(fpr, tpr, roc_threshold)
        ]
    }
 
    save_json(ROC_json_path, roc_data)
 
 
 
if __name__ == '__main__':
    args = argparse.ArgumentParser()
    args.add_argument("--config", "-c", default="configs/config.yaml")
    args.add_argument("--params", "-p", default="params.yaml")
    parsed_args = args.parse_args()
 
    try:
        logging.info("\n********************")
        logging.info(f">>>>> stage {STAGE} started <<<<<")
        main(config_path=parsed_args.config)
        logging.info(f">>>>> stage {STAGE} completed!<<<<<\n")
    except Exception as e:
        logging.exception(e)
        raise e
-----------------------------
src/stage_03_train.py

import argparse
import os
import shutil
from tqdm import tqdm
import logging
from src.utils.common import read_yaml, create_directories
import joblib
import numpy as np
from sklearn.ensemble import RandomForestClassifier
 
STAGE = "Three"
 
logging.basicConfig(
    filename=os.path.join("logs", 'running_logs.log'), 
    level=logging.INFO, 
    format="[%(asctime)s: %(levelname)s: %(module)s]: %(message)s",
    filemode="a"
    )
 
def main(config_path, params_path):
    config = read_yaml(config_path)
    params = read_yaml(params_path)
 
    artifacts = config["artifacts"]
 
    featurized_data_dir_path = os.path.join(artifacts["ARTIFACTS_DIR"], artifacts["FEATURIZED_DATA"])
    featurized_train_data_path = os.path.join(featurized_data_dir_path, artifacts["FEATURIZED_OUT_TRAIN"])
 
    model_dir_path = os.path.join(artifacts["ARTIFACTS_DIR"], artifacts["MODEL_DIR"])
    create_directories([model_dir_path])
    model_path = os.path.join(model_dir_path, artifacts["MODEL_NAME"])
 
    matrix = joblib.load(featurized_train_data_path)
 
    labels = np.squeeze(matrix[:, 1].toarray())
    X = matrix[:,2:]
 
    logging.info(f"input matrix size: {matrix.shape}")
    logging.info(f"X matrix size: {X.shape}")
    logging.info(f"Y matrix size or labels size: {labels.shape}")
 
    seed = params["train"]["seed"]
    n_est = params["train"]["n_est"]
    min_split = params["train"]['min_split']
 
    model = RandomForestClassifier(
        n_estimators=n_est, min_samples_split=min_split, n_jobs=2, random_state=seed
    )
    model.fit(X, labels)
 
    joblib.dump(model, model_path)
 
 
if __name__ == '__main__':
    args = argparse.ArgumentParser()
    args.add_argument("--config", "-c", default="configs/config.yaml")
    args.add_argument("--params", "-p", default="params.yaml")
    parsed_args = args.parse_args()
 
    try:
        logging.info("\n********************")
        logging.info(f">>>>> stage {STAGE} started <<<<<")
        main(config_path=parsed_args.config, params_path=parsed_args.params)
        logging.info(f">>>>> stage {STAGE} completed!<<<<<\n")
    except Exception as e:
        logging.exception(e)
        raise e

-----------------------------

src/stage_02_featurization.py

import argparse
import os
import shutil
from tqdm import tqdm
import logging
from src.utils.common import read_yaml, create_directories, get_df
from src.utils.featurize import save_matrix
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
 
STAGE = "Two"
 
logging.basicConfig(
    filename=os.path.join("logs", 'running_logs.log'), 
    level=logging.INFO, 
    format="[%(asctime)s: %(levelname)s: %(module)s]: %(message)s",
    filemode="a"
    )
 
def main(config_path, params_path):
    config = read_yaml(config_path)
    params = read_yaml(params_path)
 
    artifacts = config["artifacts"]
    prepared_data_dir_path = os.path.join(artifacts["ARTIFACTS_DIR"], artifacts["PREPARED_DATA"])
    train_data_path = os.path.join(prepared_data_dir_path, artifacts["TRAIN_DATA"])
    test_data_path = os.path.join(prepared_data_dir_path, artifacts["TEST_DATA"])
 
    featurized_data_dir_path = os.path.join(artifacts["ARTIFACTS_DIR"], artifacts["FEATURIZED_DATA"])
    create_directories([featurized_data_dir_path])
 
    featurized_train_data_path = os.path.join(featurized_data_dir_path, artifacts["FEATURIZED_OUT_TRAIN"])
    featurized_test_data_path = os.path.join(featurized_data_dir_path, artifacts["FEATURIZED_OUT_TEST"])
 
    max_features = params["featurize"]["max_features"]
    ngrams = params["featurize"]["ngrams"]
 
    df_train = get_df(train_data_path)
 
    train_words = np.array(df_train.text.str.lower().values.astype("U")) ## << U1000
 
    bag_of_words = CountVectorizer(
        stop_words="english", max_features=max_features, ngram_range=(1, ngrams)
    )
 
    bag_of_words.fit(train_words)
    train_words_binary_matrix = bag_of_words.transform(train_words)
 
    tfidf = TfidfTransformer(smooth_idf=False)
    tfidf.fit(train_words_binary_matrix)
    train_words_tfidf_matrix = tfidf.transform(train_words_binary_matrix)
    save_matrix(df_train, train_words_tfidf_matrix, featurized_train_data_path)
 
    df_test = get_df(test_data_path)
    test_words = np.array(df_test.text.str.lower().values.astype("U"))
    test_words_binary_matrix = bag_of_words.transform(test_words)
    test_words_tfidf_matrix = tfidf.transform(test_words_binary_matrix)
 
    save_matrix(df_test, test_words_tfidf_matrix, featurized_test_data_path)
 
 
if __name__ == '__main__':
    args = argparse.ArgumentParser()
    args.add_argument("--config", "-c", default="configs/config.yaml")
    args.add_argument("--params", "-p", default="params.yaml")
    parsed_args = args.parse_args()
 
    try:
        logging.info("\n********************")
        logging.info(f">>>>> stage {STAGE} started <<<<<")
        main(config_path=parsed_args.config, params_path=parsed_args.params)
        logging.info(f">>>>> stage {STAGE} completed!<<<<<\n")
    except Exception as e:
        logging.exception(e)
        raise e

-----------------------------
params.yaml

# This contains params to be used by the stages to train or predict
prepare:
  split: 0.3
  seed: 2021
 
featurize:
  max_features: 2500
  ngrams: 2
 
train:
  seed: 2021
  n_est: 100
  min_split: 16
-----------------------------

dvc.yaml

stages:
  prepare_data:
    cmd: python src/stage_01_prepare.py --config=configs/config.yaml --params=params.yaml
    deps:
      - src/stage_01_prepare.py
      - data/data.xml
      - src/utils/common.py
      - src/utils/data_mgmt.py
      - configs/config.yaml
    params:
      - prepare.seed
      - prepare.split
    outs:
      - artifacts/prepared/train.tsv
      - artifacts/prepared/test.tsv
  featurize:

    cmd: python src/stage_02_featurization.py --config=configs/config.yaml --params=params.yaml

    deps:

      - src/stage_02_featurization.py

      - artifacts/prepared/train.tsv

      - artifacts/prepared/test.tsv

      - src/utils/common.py

      - src/utils/featurize.py

      - configs/config.yaml

    params:

      - featurize.max_features

      - featurize.ngrams

    outs:

      - artifacts/features/train.pkl

      - artifacts/features/test.pkl
 
  train:

    cmd: python src/stage_03_train.py --config=configs/config.yaml --params=params.yaml

    deps:

      - src/stage_03_train.py

      - artifacts/features/train.pkl

      - src/utils/common.py

      - configs/config.yaml

    params:

      - train.seed

      - train.n_est

      - train.min_split

    outs:

      - artifacts/model/model.pkl
 
  evaluate:

    cmd: python src/stage_04_evaluate.py --config=configs/config.yaml

    deps:

      - src/stage_04_evaluate.py

      - artifacts/features/test.pkl

      - src/utils/common.py

      - configs/config.yaml

      - artifacts/model/model.pkl

    metrics:

      - scores.json:

          cache: false

    plots:

      - prc.json:

          cache: false

          x: recall

          y: precision

      - roc.json:

          cache: false

          x: fpr

          y: tpr
 
  run_plots_command:

    cmd: dvc plots diff

-----------------------------

Terminal :- 
=========
pip install -r requirements.txt # after adding -e . for local dependencies
dvc init
dvc repro
